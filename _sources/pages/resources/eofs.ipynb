{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de00f085-fe7a-4cc4-9dda-acf925750209",
   "metadata": {},
   "source": [
    "# Intro to EOFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc890d-7f8e-4d19-9141-75acf781cd70",
   "metadata": {},
   "source": [
    "## What are EOFs?\n",
    "\"Empirical Orthogonal Function\" (EOF) analysis is the name climate scientists use for \"principal component analysis\" (PCA). Intuitively, EOFs are the dominant spatial patterns in the data (we'll get to the precise mathematical definition of \"dominant pattern\" soon). They are \"empirical\" because they're estimated from data and \"orthogonal\" because each pattern identified in the analysis is required to be orthogonal to every other pattern (i.e., any two patterns have a spatial correlation of zero). Why use them? From a climate variability perspective, it can be helpful to think of the climate system having preferred \"modes\" of variability; in other words, preferred patterns of oscillation when perturbed{sup}`1`. Therefore a popular application of EOF analysis is to identify these \"modes\" of variability.\n",
    "\n",
    "\n",
    "{sup}`1`For what it's worth, I think about these modes like harmonics of a guitar string. When plucked, a guitar string prefers to oscillate at the fundamental frequency (1st harmonic), which has a wave length of the entire string. However, there are also \"overtones\", which correspond to oscillations at integer multiples of the fundamental frequency (e.g., 2nd harmonic has twice the frequency and half the wavelength of the 1st harmonic); see [the wikipedia page](https://en.wikipedia.org/wiki/Harmonic) for a visual representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdbf8cc-4eec-4334-a4ab-a04e6cb0dba3",
   "metadata": {},
   "source": [
    "### A visual demonstration\n",
    "Before explaining the math, let's look at a visual example. We'll start by generating some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81483437-3f84-47be-b8e4-b544c4c2ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## set plotting preferences and instantiate RNG\n",
    "sns.set(rc={\"axes.facecolor\": \"white\", \"axes.grid\": False})\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "## generate some data\n",
    "n = 100  # number of samples\n",
    "x = rng.normal(size=n)\n",
    "y = -0.4 * x + 0.25 * rng.normal(size=n)\n",
    "\n",
    "## plot the results\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## add data\n",
    "ax.scatter(x, y, s=10)\n",
    "\n",
    "## plot some vectors\n",
    "arrow_kwargs = dict(x=0, y=0, width=0.03, head_width=0.18)\n",
    "ax.arrow(dx=-0.6, dy=1, color=\"r\", **arrow_kwargs)\n",
    "ax.arrow(dx=0.5, dy=0.3, color=\"r\", **arrow_kwargs)\n",
    "ax.arrow(dx=-1, dy=0.4, color=\"k\", **arrow_kwargs)\n",
    "ax.arrow(dx=0.2, dy=0.5, color=\"k\", **arrow_kwargs)\n",
    "\n",
    "## plot axes\n",
    "ax.axvline(0, c=\"k\", ls=\"--\", lw=0.5)\n",
    "ax.axhline(0, c=\"k\", ls=\"--\", lw=0.5)\n",
    "\n",
    "## label plot\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xticks([-2, 0, 2])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f356a0-40cf-4c5a-b3c9-8109708799c4",
   "metadata": {},
   "source": [
    "I've plotted two possible sets of orthogonal vectors, one in black and one in red. Which aligns \"better\" with the data? Intuitively, you might say the black set: the longer arrow seems to be aligned along the main axis of the data. This is exactly the goal of EOFs: to find the \"direction\" or \"pattern\" which \"aligns best\" with the data. In this 2-D case (we only have two coordinates) the pattern is a 2-D vector. If we had 3 coordinates, the vector would have 3-coordinates, and so on. If we have temperature data with coordiantes of $n_{lat}$ latitude points and $n_{lon}$ longitude points, the resulting vector will be $(n_{lon}\\times n_{lat})$ elements long; if we reshape this vector back into an array with dimensions of $(n_{lon}, n_{lat})$, then we get a spatial pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12119aa-7a07-4cc9-a6b3-e736a9be545f",
   "metadata": {},
   "source": [
    "## Math behind EOFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4364800a-dc08-4b9e-a539-b037e339fb05",
   "metadata": {},
   "source": [
    "Consider a set of $n$, $m$-dimensional vectors, concatenated into a data matrix $\\mathbf{X}$:\n",
    "\\begin{align}\n",
    "    \\mathbf{X} &= \\begin{bmatrix} \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_n \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c6acc8-35f1-465c-a4f9-9ebc0cc65719",
   "metadata": {},
   "source": [
    "### What do we mean by \"dominant pattern\"?\n",
    "We want to find the \"dominant\" $m$-dimensional pattern $\\mathbf{p}$. Mathematically, we define \"dominant pattern\" as the vector of length 1 whose projection onto the data yields the largest variance in time. In more detail:\n",
    "1. First, note that the \"projection\" of a data sample $\\mathbf{x}_i$ onto the pattern is the dot product $\\mathbf{p}\\cdot\\mathbf{x}_i$ (or equivalently, in matrix form, $\\mathbf{p}^T\\mathbf{x}_i$). Intuitively, the projection is the \"similarity\" between the pattern and the sample. A high value indicates a strong positive correlation, a low value indicates a strong negative correlation, and a value close to zero indicates a weak correlation. Note we can actually compute the pattern correlation explicitly as $\\text{corr}(\\mathbf{p},\\mathbf{x}_i) = \\frac{\\mathbf{p}\\cdot\\mathbf{x}_i}{\\sqrt{\\left(\\mathbf{p}\\cdot\\mathbf{p}\\right) \\left(\\mathbf{x_i}\\cdot\\mathbf{x}_i\\right)}}$.\n",
    "2. Then we can write the projection of each datapoint onto the pattern as: $\\mathbf{q}^T = \\mathbf{p}^T\\mathbf{X}$. Note that this projection vector, $\\mathbf{q}^T$, is a $1\\times n$ row vector, with a single entry for each of the $n$ samples (columns) in the dataset.\n",
    "3. The variance of the projected dataset is given by $\\text{var}(\\mathbf{q}) = \\frac{1}{n}\\mathbf{q}^T\\mathbf{q}=\\mathbf{p}^T\\mathbf{X}\\mathbf{X}^T\\mathbf{p}$. Note that the $m\\times m$ matrix $\\mathbf{C}_{xx}\\equiv \\frac{1}{n}\\mathbf{XX}^T$ is the covariance matrix of the data! Rewriting the variance in terms of this matrix, we have: $\\text{var}(\\mathbf{q})= \\mathbf{p}^T\\mathbf{C}_{xx}\\mathbf{p}$.\n",
    "4. Finally, define the leading EOF as the vector $\\hat{\\mathbf{p}}$ which maximizes the variance of $\\mathbf{q}$ subject to the constraint that $||\\hat{\\mathbf{p}}||=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585d633-97b6-405b-9ccb-97819c4de068",
   "metadata": {},
   "source": [
    "### How to find the dominant pattern?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0691db1-d10e-4f14-8c96-c4f5ebb6da2b",
   "metadata": {},
   "source": [
    "We want to choose the pattern $\\mathbf{p}$ which maximizes $\\mathbf{p}^T\\mathbf{C}_{xx}\\mathbf{p}$ subject to the constraint $||\\mathbf{p}||=\\mathbf{p}^T\\mathbf{p}=1$. We can solve for this $\\mathbf{p}$ using a [Lagrange multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier). Defining the Lagrange multiplier as $\\lambda$, write down an \"augmented\" cost function:\n",
    "\\begin{align}\n",
    "    \\mathcal{L}(\\mathbf{p}, \\lambda) &= \\mathbf{p}^T\\mathbf{C}_{xx}\\mathbf{p} + \\lambda \\left(1-\\mathbf{p}^T\\mathbf{p}\\right).\n",
    "\\end{align}\n",
    "The first term on the LHS is the original cost function and the second term is the constraint term. Computing the gradient of the augmented cost function with respect to $\\mathbf{p}$ we have:\n",
    "\\begin{align}\n",
    "    \\frac{\\partial \\mathcal{L}}{\\partial \\mathbf{p}} &= \\mathbf{C}_{xx} \\mathbf{p} - \\lambda \\mathbf{p}\n",
    "\\end{align}\n",
    "Setting this gradient to zero, we find:\n",
    "\\begin{align}\n",
    "    \\mathbf{C}_{xx} \\mathbf{p} &= \\lambda \\mathbf{p}.\n",
    "\\end{align}\n",
    "\n",
    "(note that setting $\\frac{\\partial \\mathcal{L}}{\\partial\\lambda}=1-\\mathbf{p}^T\\mathbf{p}:=0$ yields the original constraint, $\\mathbf{p}^T\\mathbf{p}=1$). Note that to satisfy the equation above, $\\mathbf{p}$ must be an eigenvector of $\\mathbf{C}_{xx}$ (and has corresponding eigenvalue $\\lambda$). Note that if $\\mathbf{p}$ is an eigenvector with eigenvalue $\\lambda$ and length 1, then we have $\\mathbf{p}^T\\mathbf{C}_{xx}\\mathbf{p} = \\mathbf{p}^T\\left(\\lambda\\mathbf{p}\\right)=\\lambda$. Therefore, the pattern which maximizes $\\mathbf{p}^T\\mathbf{C}_{xx}\\mathbf{p}$ is the eigenvector of $\\mathbf{C}_{xx}$ with the largest eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa12edb-a703-47f4-8c3c-b7065595eb55",
   "metadata": {},
   "source": [
    "#### Connection to SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eec9c79-40f1-408e-8f83-27eb706118b7",
   "metadata": {},
   "source": [
    "Before explaining how to compute the projection timeseries and explained variance of each pattern, let's look at the connection between the eigenvectors of $\\mathbf{C}_{xx}$ and the singular value decomposition (SVD) of $\\mathbf{X}$. Let's start by writing $\\mathbf{X}$ in terms of its singular value decomposition:\n",
    "\\begin{align}\n",
    "    \\mathbf{X}&=\\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{V}^T,\n",
    "\\end{align}\n",
    "Then the covariance matrix can be written as:\n",
    "\\begin{align}\n",
    "    \\mathbf{C}_{xx} &=\\mathbf{XX}^T\\\\\n",
    "    &= \\left(\\mathbf{U}\\boldsymbol{\\Lambda} \\mathbf{V}^T\\right)\\left(\\mathbf{V} \\boldsymbol{\\Lambda} \\mathbf{U}^T\\right)\\\\\n",
    "    &= \\mathbf{U}\\boldsymbol{\\Lambda}^2\\mathbf{U}^T\n",
    "\\end{align}\n",
    "Where we used the property of orthogonal matrices $\\mathbf{V}^T\\mathbf{V}=\\mathbf{I}$. Next, right-multiply by $\\mathbf{U}$:\n",
    "\\begin{align}\n",
    "    \\mathbf{C}_{xx}\\mathbf{U} &= \\mathbf{U}\\boldsymbol{\\Lambda}^2\n",
    "\\end{align}\n",
    "Therefore, the left singular vectors of $\\mathbf{X}$ are eigenvectors of $\\mathbf{C}_{xx}$. In particular, the $i^{th}$ left singular vector satisfies the eigenvector equation $\\mathbf{C}_{xx}\\mathbf{u}_i = \\lambda_i^2\\mathbf{u}_i$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a65d0-1545-474e-9e58-ebc5b235c8f9",
   "metadata": {},
   "source": [
    "### Projection timeseries and explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1291ac-5e38-4261-a7e3-31f7e65f9a8f",
   "metadata": {},
   "source": [
    "The principal component timeseries is given by projecting the data onto the pattern: \n",
    "\\begin{align}\n",
    "    \\mathbf{q}_i^T &= \\mathbf{p}_i^T\\mathbf{X}\n",
    "\\end{align}\n",
    "Setting $\\mathbf{p}_i$ to the $i^{th}$ singular vector of $\\mathbf{X}$, we have:\n",
    "\\begin{align}\n",
    "    \\mathbf{q}_i^T &=\\mathbf{u}_i^T\\mathbf{X}\\\\\n",
    "    &= \\mathbf{u}_i^T\\left(\\mathbf{U}\\boldsymbol{\\Lambda}\\mathbf{V}^T\\right)\\\\\n",
    "    &=\\sum_{j}\\mathbf{u}_i\\mathbf{u}_j\\lambda_j\\mathbf{v}_j^T\\\\\n",
    "    &= \\lambda_i\\mathbf{v}_i^T,\n",
    "\\end{align}\n",
    "where the last equality comes from noticing that $\\mathbf{u}_i^T\\mathbf{u}_j=1$ if $i=j$ and 0 otherwise. That is, the projection of the data onto the pattern is given by the right singular vectors, scaled by the singular values. Note that the variance of the data's projection onto $\\mathbf{u}_i$ is then given by:\n",
    "\\begin{align}\n",
    "    \\text{var}(\\mathbf{q}_i) &= \\left(\\lambda_i\\mathbf{v}_i^T\\right)\\left(\\lambda_i\\mathbf{v}_i^T\\right)^T\\\\\n",
    "    &=\\lambda_i^2\\mathbf{v}_i^T\\mathbf{v}_i\\\\\n",
    "    &=\\lambda_i^2,\n",
    "\\end{align}\n",
    "and the fraction of variance explained by the $i^{th}$ singular vector is given by:\n",
    "\\begin{align}\n",
    "    \\frac{\\text{var}(\\mathbf{q}_i)}{\\sum_j \\text{var}(\\mathbf{q}_j)} &= \\frac{\\lambda_i^2}{\\sum_j\\lambda_j^2}\n",
    "\\end{align}\n",
    "\n",
    "**In summary,** if we write $\\mathbf{X}$ in terms of its SVD as $\\mathbf{X}=\\mathbf{USV}^T$, we have:\n",
    "- The columns of $\\mathbf{U}$ are the EOFs (spatial patterns)\n",
    "- The columns of $\\mathbf{V}$ (or rows of $\\mathbf{V}^T$) are the corresponding timeseries\n",
    "- The fraction of total variance explained by the data's projection onto the $i^{th}$ EOF is given by $\\frac{\\lambda_i^2}{\\sum_j \\lambda_j^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be215b5-d395-4fba-b577-55c2c4ebd7c1",
   "metadata": {},
   "source": [
    "## Examples with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808fb06-fd35-4697-a0a0-a0030ee0de19",
   "metadata": {},
   "source": [
    "Here we'll use the ```xeofs``` package to compute EOFs (rather than doing the SVD manually)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50199d-c70a-4f9f-8b91-b6d70dff4fc9",
   "metadata": {},
   "source": [
    "First, some functions to load, detrend, and plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58125b-9a1b-48af-b211-cd55b48df371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "import cmocean\n",
    "\n",
    "\n",
    "def trim(data):\n",
    "    \"\"\"trim to lon/lat range\"\"\"\n",
    "\n",
    "    ## subset data in space (and downsample to 1-deg res.)\n",
    "    data_trimmed = data.sel(longitude=slice(280, 360, 4), latitude=slice(80, 20, 4))\n",
    "\n",
    "    ## get winter means\n",
    "    data_trimmed_winter = (\n",
    "        slp.resample(time=\"QS-DEC\").mean().isel(time=slice(None, None, 4))\n",
    "    )\n",
    "\n",
    "    return data_trimmed_winter\n",
    "\n",
    "\n",
    "def load_data(server_fp=None):\n",
    "    \"\"\"Load data from local directory if available; otherwise from server\"\"\"\n",
    "\n",
    "    ## filepath for saving data locally\n",
    "    save_fp = pathlib.Path(\"./slp_data.nc\")\n",
    "\n",
    "    ## check if data is saved locally\n",
    "    if save_fp.is_file():\n",
    "\n",
    "        data = xr.open_dataset(\"slp_data.nc\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        ## filepath of ERA5 data on server\n",
    "        fp_on_server = pathlib.Path(\n",
    "            \"cmip6/data/era5/reanalysis/single-levels/monthly-means/mean_sea_level_pressure\"\n",
    "        )\n",
    "\n",
    "        ## get full filepath\n",
    "        fp = pathlib.Path(server_fp) / fp_on_server\n",
    "\n",
    "        ## open data\n",
    "        data = xr.open_mfdataset(fp.glob(\"*.nc\"))\n",
    "\n",
    "        ## trim data\n",
    "        data = trim(data)\n",
    "\n",
    "        ## save to file\n",
    "        data.to_netcdf(save_fp)\n",
    "\n",
    "    ## convert data from Pa to hPa\n",
    "    hPa_per_Pa = 1 / 100\n",
    "    data_hPa = data * hPa_per_Pa\n",
    "\n",
    "    return data_hPa[\"msl\"]\n",
    "\n",
    "\n",
    "def get_trend_coefs(data, dim=\"time\", deg=1):\n",
    "    \"\"\"get coefficients for trend\"\"\"\n",
    "    return data.polyfit(dim=dim, deg=deg)[\"polyfit_coefficients\"]\n",
    "\n",
    "\n",
    "def get_trend(data, dim=\"time\", deg=1):\n",
    "    \"\"\"\n",
    "    Get trend for an xr.dataarray along specified dimension,\n",
    "    by fitting polynomial of degree 'deg'.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Get coefficients for best fit\n",
    "    polyfit_coefs = get_trend_coefs(data=data, dim=dim, deg=deg)\n",
    "\n",
    "    ## Get best fit line (linear trend in this case)\n",
    "    trend = xr.polyval(data[dim], polyfit_coefs)\n",
    "\n",
    "    return trend\n",
    "\n",
    "\n",
    "def detrend(data, dim=\"time\", deg=1):\n",
    "    \"\"\"\n",
    "    Remove trend of degree 'deg' from data, along dimension 'dim'.\n",
    "    \"\"\"\n",
    "\n",
    "    return data - get_trend(data, dim=dim, deg=deg)\n",
    "\n",
    "\n",
    "def plot_setup(fig, projection, lon_range, lat_range, xticks=None, yticks=None):\n",
    "    \"\"\"Add a subplot to the figure with the given map projection\n",
    "    and lon/lat range. Returns an Axes object.\"\"\"\n",
    "\n",
    "    ## increase resolution for projection\n",
    "    ## (otherwise lines plotted on surface won't follow curved trajectories)\n",
    "    projection.threshold /= 1000\n",
    "\n",
    "    ## Create subplot with given projection\n",
    "    ax = fig.add_subplot(projection=projection)\n",
    "\n",
    "    ## Subset to given region\n",
    "    extent = [*lon_range, *lat_range]\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "    ## draw coastlines\n",
    "    ax.coastlines(linewidths=0.5)\n",
    "\n",
    "    ## add tick labels\n",
    "    if xticks is not None:\n",
    "\n",
    "        ## add lon/lat labels\n",
    "        gl = ax.gridlines(\n",
    "            draw_labels=True,\n",
    "            linestyle=\"-\",\n",
    "            alpha=0.1,\n",
    "            linewidth=0.5,\n",
    "            color=\"k\",\n",
    "            zorder=1.05,\n",
    "        )\n",
    "\n",
    "        ## specify which axes to label\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "\n",
    "        ## specify ticks\n",
    "        gl.ylocator = mticker.FixedLocator(yticks)\n",
    "        gl.xlocator = mticker.FixedLocator(xticks)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_setup_atlantic(fig):\n",
    "    \"\"\"Plot Atlantic region\"\"\"\n",
    "\n",
    "    ## adjust figure size\n",
    "    fig.set_size_inches(5, 3)\n",
    "\n",
    "    ## specify map projection\n",
    "    proj = ccrs.Orthographic(central_longitude=-40, central_latitude=50)\n",
    "\n",
    "    ## get ax object\n",
    "    ax = plot_setup(\n",
    "        fig,\n",
    "        proj,\n",
    "        lon_range=[200, 360],\n",
    "        lat_range=[20, 80],\n",
    "        xticks=[150, -160, -110],\n",
    "        yticks=[-20, 0, 20],\n",
    "    )\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa6df6-a9e7-4ec9-b030-6f640d49abe5",
   "metadata": {},
   "source": [
    "Now, load the data and compute anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744e6d6-24f4-4f02-b9ad-67d36eb64a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load all data and detrend\n",
    "slp = load_data(server_fp=\"/Volumes\")\n",
    "slp_anom = detrend(slp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ffd82-241a-46cb-a454-4a1cf31ec3d8",
   "metadata": {},
   "source": [
    "### 2-D example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd2946-3193-4d5e-8750-374ba837cc8b",
   "metadata": {},
   "source": [
    "Let's look at the SLP anomalies near Iceland and the Azores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44decb9c-2762-44c8-8ec4-184e5690736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xeofs as xe\n",
    "import pandas as pd\n",
    "\n",
    "## get SLP over iceland and azores\n",
    "slp_ice = slp_anom.interp(longitude=342.5, latitude=65)\n",
    "slp_azo = slp_anom.interp(longitude=332, latitude=38.5)\n",
    "\n",
    "## put iceland/azores data in single array\n",
    "slp_2d = xr.concat([slp_ice, slp_azo], dim=pd.Index([\"Iceland\", \"Azores\"], name=\"posn\"))\n",
    "\n",
    "## compute eofs\n",
    "model = xe.single.EOF(use_coslat=False)\n",
    "model.fit(slp_2d, dim=\"time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010ab11-445e-4596-8b3c-c89ace08ea6e",
   "metadata": {},
   "source": [
    "Let's check the package gives the expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26000157-b809-4224-91c0-9108b3d2c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute svd\n",
    "u, s, vt = np.linalg.svd(slp_2d.values)\n",
    "\n",
    "## compare first pattern to package results\n",
    "## (sign doesn't matter, so check pos. or neg. match)\n",
    "print(\n",
    "    np.allclose(u[:, 0], model.components().isel(mode=0))\n",
    "    | np.allclose(-u[:, 0], model.components().isel(mode=0))\n",
    ")\n",
    "\n",
    "## compare explained variance ratio\n",
    "print(np.allclose(s**2 / (s**2).sum(), model.explained_variance_ratio()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604825e-7a98-4d19-a33f-602739959bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the results\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## add data\n",
    "ax.scatter(slp_2d.sel(posn=\"Iceland\"), slp_2d.sel(posn=\"Azores\"), s=10)\n",
    "\n",
    "## plot EOFs\n",
    "arrow_kwargs = dict(x=0, y=0, width=0.15, head_width=0.9)\n",
    "\n",
    "# 1st EOF\n",
    "ax.arrow(\n",
    "    dx=10 * model.components().isel(mode=0, posn=0),\n",
    "    dy=10 * model.components().isel(mode=0, posn=1),\n",
    "    color=\"k\",\n",
    "    **arrow_kwargs\n",
    ")\n",
    "\n",
    "# 2nd EOF\n",
    "ax.arrow(\n",
    "    dx=5 * model.components().isel(mode=1, posn=0),\n",
    "    dy=5 * model.components().isel(mode=1, posn=1),\n",
    "    color=\"k\",\n",
    "    **arrow_kwargs\n",
    ")\n",
    "\n",
    "## plot axes\n",
    "ax.axvline(0, c=\"k\", ls=\"--\", lw=0.5)\n",
    "ax.axhline(0, c=\"k\", ls=\"--\", lw=0.5)\n",
    "\n",
    "## label plot\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(\"Iceland SLP anomaly (hPa)\")\n",
    "ax.set_ylabel(\"Azores SLP anomaly (hPa)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b935641-7892-4960-90b3-649327ea3083",
   "metadata": {},
   "source": [
    "### Full spatial field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0b306-9bdd-4734-b7f1-7a4471c36733",
   "metadata": {},
   "source": [
    "Compute EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59807d-9e6a-4402-b31b-2201412ad05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute eofs\n",
    "model = xe.single.EOF(use_coslat=True, n_modes=10)\n",
    "model.fit(slp_anom, dim=\"time\")\n",
    "\n",
    "## extract eofs and principal component timeseries ('pcs') for convenience\n",
    "eofs = model.components()\n",
    "pcs = model.scores()\n",
    "\n",
    "## check that we can obtain 'pcs' using model.transform function\n",
    "print(np.allclose(pcs, model.transform(slp_anom)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f787c-fc9b-48a4-9608-5af6a515bbeb",
   "metadata": {},
   "source": [
    "Plot spatial pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1bca45-a9b9-4055-a476-fb35d4d3f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "fig, ax = plot_setup_atlantic(fig)\n",
    "\n",
    "ax.pcolormesh(\n",
    "    eofs.longitude,\n",
    "    eofs.latitude,\n",
    "    eofs.isel(mode=0),\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap=\"cmo.balance\",\n",
    "    vmax=0.03,\n",
    "    vmin=-0.03,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa56cb-2478-48ef-b996-d0dd21b11c47",
   "metadata": {},
   "source": [
    "Plot timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e7dec-310b-4025-9ebf-a63225211c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## plot the data\n",
    "ax.plot(pcs.time.dt.year, pcs.isel(mode=0))\n",
    "\n",
    "## label\n",
    "ax.set_ylabel(\"Score (unitless)\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a367a-fc79-4122-a2f5-60a87c25eb94",
   "metadata": {},
   "source": [
    "Explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c5fd3-a22f-4d68-9014-7dea5a18f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## plot the data\n",
    "ax.bar(model.explained_variance_ratio().mode, model.explained_variance_ratio())\n",
    "\n",
    "## label\n",
    "ax.set_ylabel(\"Fraction\")\n",
    "ax.set_xlabel(\"Mode #\")\n",
    "ax.set\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
