{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a136808-d55d-4ae8-8e93-588cc10d69de",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9700d0-e991-4a5a-b324-042d0233a43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import cmocean\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as mticker\n",
    "import scipy.signal\n",
    "import copy\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "## (optional) remove gridlines from plots\n",
    "sns.set(rc={\"axes.facecolor\": \"white\", \"axes.grid\": False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0dd04-24c1-45f4-ba79-de42d467d9f1",
   "metadata": {},
   "source": [
    "# Helper fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e8f2ad-ed53-458c-900d-c43b9c211059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_setup(fig, projection, lon_range, lat_range, xticks=None, yticks=None):\n",
    "    \"\"\"Add a subplot to the figure with the given map projection\n",
    "    and lon/lat range. Returns an Axes object.\"\"\"\n",
    "\n",
    "    ## increase resolution for projection\n",
    "    ## (otherwise lines plotted on surface won't follow curved trajectories)\n",
    "    projection.threshold /= 1000\n",
    "\n",
    "    ## Create subplot with given projection\n",
    "    ax = fig.add_subplot(projection=projection)\n",
    "\n",
    "    ## Subset to given region\n",
    "    extent = [*lon_range, *lat_range]\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "    ## draw coastlines\n",
    "    ax.coastlines(linewidths=0.5)\n",
    "\n",
    "    ## add tick labels\n",
    "    if xticks is not None:\n",
    "\n",
    "        ## add lon/lat labels\n",
    "        gl = ax.gridlines(\n",
    "            draw_labels=True,\n",
    "            linestyle=\"-\",\n",
    "            alpha=0.1,\n",
    "            linewidth=0.5,\n",
    "            color=\"k\",\n",
    "            zorder=1.05,\n",
    "        )\n",
    "\n",
    "        ## specify which axes to label\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "\n",
    "        ## specify ticks\n",
    "        gl.ylocator = mticker.FixedLocator(yticks)\n",
    "        gl.xlocator = mticker.FixedLocator(xticks)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_box_outline(ax, lon_range, lat_range, c=\"k\"):\n",
    "    \"\"\"\n",
    "    Plot box outlining the specifed lon/lat range on given\n",
    "    ax object.\n",
    "    \"\"\"\n",
    "\n",
    "    ## get width and height\n",
    "    height = lat_range[1] - lat_range[0]\n",
    "    width = lon_range[1] - lon_range[0]\n",
    "\n",
    "    ## add rectangle to plot\n",
    "    ax.add_patch(\n",
    "        mpatches.Rectangle(\n",
    "            xy=[lon_range[0], lat_range[0]],\n",
    "            height=height,\n",
    "            width=width,\n",
    "            transform=ccrs.PlateCarree(),\n",
    "            facecolor=\"none\",\n",
    "            edgecolor=c,\n",
    "            linewidth=1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_correlation(plot_setup_fn, corr, x, y):\n",
    "    \"\"\"\n",
    "    Make spatial plot of correlation, using the specified\n",
    "    plot setup function and pre-computed correlation.\n",
    "    Args:\n",
    "        - plot_setup_fn: function that returns a fig, ax object\n",
    "        - corr: xarray with spatial correlation\n",
    "        - x, y: lon/lat points for plotting\n",
    "    \"\"\"\n",
    "\n",
    "    ## blank canvas to plot on\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ## draw background map of Atlantic\n",
    "    fig, ax = plot_setup_fn(fig)\n",
    "\n",
    "    ## plot the data\n",
    "    plot_data = ax.contourf(\n",
    "        x,\n",
    "        y,\n",
    "        corr,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        levels=make_cb_range(1, 0.1),\n",
    "        extend=\"both\",\n",
    "        cmap=\"cmo.balance\",\n",
    "    )\n",
    "\n",
    "    ## create colorbath\n",
    "    colorbar = fig.colorbar(plot_data, label=\"Corr.\", ticks=[-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_setup_pacific(fig):\n",
    "    \"\"\"Plot Atlantic region\"\"\"\n",
    "\n",
    "    ## adjust figure size\n",
    "    fig.set_size_inches(5, 3)\n",
    "\n",
    "    ## specify map projection\n",
    "    proj = ccrs.PlateCarree(central_longitude=-160)\n",
    "\n",
    "    ## get ax object\n",
    "    ax = plot_setup(\n",
    "        fig,\n",
    "        proj,\n",
    "        lon_range=[100, 300],\n",
    "        lat_range=[-30, 30],\n",
    "        xticks=[150, -160, -110],\n",
    "        yticks=[-20, 0, 20],\n",
    "    )\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def make_cb_range(amp, delta):\n",
    "    \"\"\"Make colorbar_range for cmo.balance\n",
    "    Args:\n",
    "        - 'amp': amplitude of maximum value for colorbar\n",
    "        - 'delta': increment for colorbar\n",
    "    \"\"\"\n",
    "    return np.concatenate(\n",
    "        [np.arange(-amp, 0, delta), np.arange(delta, amp + delta, delta)]\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_setup_timeseries():\n",
    "    \"\"\"\n",
    "    Create fig, ax objects and label time axis\n",
    "    \"\"\"\n",
    "\n",
    "    ## set up plot\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "    ## restrict to last 50 years and label axes\n",
    "    ax.set_xlim([datetime.date(1970, 1, 1), None])\n",
    "\n",
    "    ax.set_xticks(\n",
    "        [\n",
    "            datetime.date(1979, 1, 1),\n",
    "            datetime.date(2000, 6, 30),\n",
    "            datetime.date(2021, 12, 31),\n",
    "        ]\n",
    "    )\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y\"))\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def plot_seasonal_cycle(mean, std):\n",
    "    \"\"\"\n",
    "    Plot the seasonal cycle (monthly mean ± 1 standard dev.)\n",
    "    \"\"\"\n",
    "\n",
    "    ## plot\n",
    "    fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "    ## mean\n",
    "    ax.plot(np.arange(1, 13), mean, c=\"k\", label=r\"$\\mu$\")\n",
    "\n",
    "    ## mean ± std\n",
    "    ax.plot(np.arange(1, 13), mean + std, c=\"k\", lw=0.5, label=r\"$\\mu \\pm \\sigma$\")\n",
    "    ax.plot(np.arange(1, 13), mean - std, c=\"k\", lw=0.5)\n",
    "\n",
    "    ## label\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "def spatial_avg(data):\n",
    "    \"\"\"function to compute spatial average of data on grid with constant\n",
    "    longitude/latitude spacing.\"\"\"\n",
    "\n",
    "    ## first, compute cosine of latitude (after converting degrees to radians)\n",
    "    latitude_radians = np.deg2rad(data.latitude)\n",
    "    cos_lat = np.cos(latitude_radians)\n",
    "\n",
    "    ## get weighted average using xarray\n",
    "    avg = data.weighted(weights=cos_lat).mean([\"longitude\", \"latitude\"])\n",
    "\n",
    "    return avg\n",
    "\n",
    "\n",
    "def get_trend_coefs(data, dim=\"time\", deg=1):\n",
    "    \"\"\"get coefficients for trend\"\"\"\n",
    "    return data.polyfit(dim=dim, deg=deg)[\"polyfit_coefficients\"]\n",
    "\n",
    "\n",
    "def get_trend(data, dim=\"time\", deg=1):\n",
    "    \"\"\"\n",
    "    Get trend for an xr.dataarray along specified dimension,\n",
    "    by fitting polynomial of degree 'deg'.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Get coefficients for best fit\n",
    "    polyfit_coefs = get_trend_coefs(data=data, dim=dim, deg=deg)\n",
    "\n",
    "    ## Get best fit line (linear trend in this case)\n",
    "    trend = xr.polyval(data[dim], polyfit_coefs)\n",
    "\n",
    "    return trend\n",
    "\n",
    "\n",
    "def detrend(data, dim=\"time\", deg=1):\n",
    "    \"\"\"\n",
    "    Remove trend of degree 'deg' from data, along dimension 'dim'.\n",
    "    \"\"\"\n",
    "\n",
    "    return data - get_trend(data, dim=dim, deg=deg)\n",
    "\n",
    "\n",
    "def get_empirical_pdf(x, bin_edges=None):\n",
    "    \"\"\"\n",
    "    Estimate the \"empirical\" probability distribution function for the data x.\n",
    "    In this case the result is a normalized histogram,\n",
    "    Normalized means that integrating over the histogram yields 1.\n",
    "    Returns the PDF (normalized histogram) and edges of the histogram bins\n",
    "    \"\"\"\n",
    "\n",
    "    ## compute histogram\n",
    "    if bin_edges is None:\n",
    "        hist, bin_edges = np.histogram(x)\n",
    "\n",
    "    else:\n",
    "        hist, _ = np.histogram(x, bins=bin_edges)\n",
    "\n",
    "    ## normalize to a probability distribution (PDF)\n",
    "    bin_width = bin_edges[1:] - bin_edges[:-1]\n",
    "    pdf = hist / (hist * bin_width).sum()\n",
    "\n",
    "    return pdf, bin_edges\n",
    "\n",
    "\n",
    "def get_gaussian_best_fit(x):\n",
    "    \"\"\"Get gaussian best fit to data, and evaluate\n",
    "    probabilities over the range of the data.\"\"\"\n",
    "\n",
    "    ## get normal distribution best fit\n",
    "    gaussian = scipy.stats.norm(loc=x.mean(), scale=x.std())\n",
    "\n",
    "    ## evaluate over range of data\n",
    "    amp = np.max(np.abs(x.values))\n",
    "    x_eval = np.linspace(-amp, amp)\n",
    "    pdf_eval = gaussian.pdf(x_eval)\n",
    "\n",
    "    return pdf_eval, x_eval\n",
    "\n",
    "\n",
    "def swap_longitude_range(data):\n",
    "    \"\"\"swap longitude range of xr.DataArray from [0,360) to (-180, 180]\"\"\"\n",
    "\n",
    "    ## copy of longitude coordinate to be modified\n",
    "    new_longitude = copy.deepcopy(data.lon.values)\n",
    "\n",
    "    ## find index where longitude first exceeds 180.\n",
    "    ## (note: np.argmax returns first instance of \"True\" in boolean array)\n",
    "    swap_idx = np.argmax(new_longitude > 180)\n",
    "\n",
    "    ## relabel values >180\n",
    "    new_longitude[swap_idx:] = -360 + new_longitude[swap_idx:]\n",
    "\n",
    "    ## add this coordinate back to the array\n",
    "    data[\"lon\"] = new_longitude\n",
    "\n",
    "    ## \"roll\" the data to be centered at zero\n",
    "    data = data.roll({\"lon\": -swap_idx}, roll_coords=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_autocorr_helper(x, lag, month=None):\n",
    "    \"\"\"Get autocorrelation of data for single lag\"\"\"\n",
    "\n",
    "    ## return 1 for a lag of 0\n",
    "    if lag == 0:\n",
    "        return 1.0\n",
    "\n",
    "    ## get lagged version of x\n",
    "    elif lag > 0:\n",
    "        x_lagged = x.isel(time=slice(lag, None))\n",
    "        x_ = x.isel(time=slice(None, -lag))\n",
    "\n",
    "    else:\n",
    "        x_lagged = x.isel(time=slice(None, lag))\n",
    "        x_ = x.isel(time=slice(-lag, None))\n",
    "\n",
    "    ## re-label time axis so arrays match\n",
    "    x_lagged[\"time\"] = x_.time\n",
    "\n",
    "    ## subset for data from given month\n",
    "    if month is not None:\n",
    "        is_month = x_.time.dt.month == month\n",
    "        x_ = x_.isel(time=is_month)\n",
    "        x_lagged = x_lagged.isel(time=is_month)\n",
    "\n",
    "    return get_corr_coef(x_, x_lagged).item()\n",
    "\n",
    "\n",
    "def get_autocorr(x, lags, month=None):\n",
    "    \"\"\"Get autocorrelation for data for multiple lags\"\"\"\n",
    "\n",
    "    ## put autocorrelation for each lag in array\n",
    "    autocorr = [get_autocorr_helper(x, lag, month) for lag in lags]\n",
    "\n",
    "    ## convert to xr.DataArray\n",
    "    return xr.DataArray(autocorr, coords={\"lag\": lags})\n",
    "\n",
    "\n",
    "def get_autocorr_by_month(x, lags):\n",
    "    \"\"\"Get autocorrelation for each month, and stack in array\"\"\"\n",
    "\n",
    "    ## compute autocorrelation for each month\n",
    "    autocorr = [get_autocorr(x, lags, month=m) for m in np.arange(1, 13)]\n",
    "\n",
    "    ## convert to xarray\n",
    "    return xr.concat(autocorr, dim=pd.Index(np.arange(1, 13), name=\"month\"))\n",
    "\n",
    "\n",
    "def load_simulation(varname, member_id, simulation_type, preprocess_func=None):\n",
    "    \"\"\"\n",
    "    Load dataset for single simulation, for single variable.\n",
    "    Arguments:\n",
    "        - varname: name of variable to load, one of {\"SST\",\"PSL\"}\n",
    "        - member_id: ID of ensemble member to load, an integer in the range [1,10]\n",
    "        - simulation_type: one of {\"hist\", \"rcp85\"}\n",
    "        - preprocess func: optional preprocessing function to apply to the simulation\n",
    "    Returns:\n",
    "        - xarray dataarray with given data\n",
    "    \"\"\"\n",
    "\n",
    "    ## Filepath to the CESM LENS dataset\n",
    "    # lens_fp = pathlib.Path(\"cmip6/data/cmip6/CMIP/NCAR/LENS\")\n",
    "    lens_fp = pathlib.Path(\"jetstream/climate/data1/yokwon/CESM1_LE/downloaded/ocn/proc/tseries/monthly\")\n",
    "\n",
    "    #### 1. get filepath to data\n",
    "    data_fp = SERVER_FP / lens_fp / pathlib.Path(varname)\n",
    "\n",
    "    #### 2. get naming pattern for files to open\n",
    "    if simulation_type == \"hist\":\n",
    "        # file_pattern = f\"*20TRC*.{member_id:03d}.*.nc\"\n",
    "        file_pattern = f\"*B20TRC5CNBDRD*g16.{member_id:03d}*.nc\"\n",
    "\n",
    "    elif simulation_type == \"rcp85\":\n",
    "        file_pattern = f\"*RCP85*.{member_id:03d}.*.nc\"\n",
    "\n",
    "    else:\n",
    "        print(\"Not a valid simulation type\")\n",
    "\n",
    "    #### 3. open the relevant datasets, applying preprocessing function\n",
    "    data = xr.open_mfdataset(\n",
    "        paths=data_fp.glob(file_pattern),\n",
    "        preprocess=preprocess_func,\n",
    "        chunks={\"time\": 60},\n",
    "    )\n",
    "\n",
    "    return data[[varname,\"TAREA\"]].squeeze(drop=True)\n",
    "\n",
    "\n",
    "def load_ensemble_helper(varname, simulation_type, preprocess_func=None):\n",
    "    \"\"\"\n",
    "    Load all ensemble members for given simulation type and variable.\n",
    "    Arguments:\n",
    "        - varname: name of variable to load, one of {\"SST\",\"PSL\"}\n",
    "        - simulation_type: one of {\"hist\", \"rcp85\"}\n",
    "        - preprocess func: optional preprocessing function to apply to the simulation\n",
    "    Returns:\n",
    "        - xarray dataarray with given data and 'ensemble' dimension\n",
    "    \"\"\"\n",
    "\n",
    "    ## put arguments in dictionary\n",
    "    kwargs = dict(\n",
    "        varname=varname,\n",
    "        simulation_type=simulation_type,\n",
    "        preprocess_func=preprocess_func,\n",
    "    )\n",
    "\n",
    "    ## put results in list\n",
    "    member_ids = np.concatenate([np.arange(1,36), np.arange(101,108)])\n",
    "    data = [load_simulation(member_id=i, **kwargs) for i in member_ids]\n",
    "\n",
    "    ## concatenate data along the \"ensemble\" dimension\n",
    "    ensemble_dim = pd.Index(np.arange(1, 43), name=\"member\")\n",
    "    data = xr.concat(data, dim=ensemble_dim)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_ensemble(varname, simulation_type, lat_range, preprocess_func=None, save_fp=None):\n",
    "    \"\"\"\n",
    "    Load all ensemble members for given simulation type and variable.\n",
    "    (Checks if data exists locally first).\n",
    "    Arguments:\n",
    "        - varname: name of variable to load, one of {\"SST\",\"PSL\"}\n",
    "        - simulation_type: one of {\"hist\", \"rcp85\"}\n",
    "        - preprocess func: optional preprocessing function to apply to the simulation\n",
    "        - save_fp: pathlib.Path object (save the result here if specified)\n",
    "    Returns:\n",
    "        - xarray dataarray with given data and 'ensemble' dimension\n",
    "    \"\"\"\n",
    "\n",
    "    ## define preprocessing function which trims to specified latitude range\n",
    "    preprocess_func_helper = lambda data : preprocess_func(data, lat_range=lat_range)\n",
    "\n",
    "    ## put arguments in dictionary\n",
    "    kwargs = dict(\n",
    "        varname=varname,\n",
    "        simulation_type=simulation_type,\n",
    "        preprocess_func=preprocess_func_helper,\n",
    "    )\n",
    "\n",
    "    ## load pre-computed data if it exists\n",
    "    if save_fp is not None:\n",
    "\n",
    "        ## path to file\n",
    "        save_fp = save_fp / f\"{varname}_{simulation_type}_{lat_range[0]}_to_{lat_range[1]}.nc\"\n",
    "\n",
    "        ## check if file exists:\n",
    "        if save_fp.is_file():\n",
    "            data = xr.open_dataset(save_fp)\n",
    "\n",
    "        else:\n",
    "\n",
    "            ## load the data and save to file for next time\n",
    "            data = load_ensemble_helper(**kwargs)\n",
    "\n",
    "            print(\"saving to file\")\n",
    "            data.to_netcdf(save_fp)\n",
    "\n",
    "    else:\n",
    "\n",
    "        ## don't load/save the data\n",
    "        data = load_ensemble_helper(**kwargs)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess(data, lat_range):\n",
    "    \"\"\"\n",
    "    Preprocessing steps:\n",
    "        1. remove data before Feb 1920\n",
    "        2. trim in lon/lat space\n",
    "        3. convert time dimension from cftime to datetime\n",
    "\n",
    "        Note: assumes varname is \"SST\"!\n",
    "    \"\"\"\n",
    "\n",
    "    ## trim in time\n",
    "    data_ = data.sel(time=slice(\"1920-02\", None))\n",
    "\n",
    "    ## trim in space\n",
    "    data_ = trim(data_, lat_range=lat_range)\n",
    "\n",
    "    ## update time dimension\n",
    "    start_year = data_.time.isel(time=0).dt.year.item()\n",
    "    start_month = data_.time.isel(time=0).dt.month.item()\n",
    "    start_date = f\"{start_year}-{start_month}-01\"\n",
    "    data_[\"time\"] = pd.date_range(start=start_date, periods=len(data_.time), freq=\"MS\")\n",
    "\n",
    "    ## compute spatial average (weighted by grid cell area)\n",
    "    data_[\"SST\"] = data_[\"SST\"].weighted(data_[\"TAREA\"]).mean([\"nlat\",\"nlon\"])\n",
    "\n",
    "    return data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e00ced0-49fb-493c-9caa-4ce21aeb7073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_longitude_range_TLONG(data):\n",
    "    \"\"\"swap longitude range of xr.DataArray from [0,360) to (-180, 180].\n",
    "    Handles case with 2-dimension longitude coordinates ('TLONG')\"\"\"\n",
    "\n",
    "    ## make copy of longitude coordinate to be modified\n",
    "    TLONG_new = copy.deepcopy(data.TLONG.values)\n",
    "\n",
    "    ## relabel values greater than 180\n",
    "    exceeds_180 = TLONG_new > 180\n",
    "    TLONG_new[exceeds_180] = -360 + TLONG_new[exceeds_180]\n",
    "\n",
    "    ## Update the coordinate on the xarray object\n",
    "    data[\"TLONG\"].values = TLONG_new\n",
    "\n",
    "    ## next, transpose data so that longitude is last dimension\n",
    "    ## (we'll do all the sorting along this dimension)\n",
    "    data = data.transpose(..., \"nlon\")\n",
    "\n",
    "    ## Get indices needed to sort longitude to be monotonic increasing\n",
    "    TLONG_sort_idx = np.argsort(data[\"TLONG\"].values, axis=-1)\n",
    "\n",
    "    ## sort the lon/lat coordindates\n",
    "    sort = lambda X, idx: np.take_along_axis(X.values, indices=idx, axis=-1)\n",
    "    data[\"TLONG\"].values = sort(data[\"TLONG\"], idx=TLONG_sort_idx)\n",
    "    data[\"TLAT\"].values = sort(data[\"TLAT\"], idx=TLONG_sort_idx)\n",
    "\n",
    "    #### sort the SST data\n",
    "\n",
    "    # first, check to see if data has more than two dimensions\n",
    "    \n",
    "    if data.ndim > 2:\n",
    "        extra_dims = [i for i in range(data.ndim - 2)]\n",
    "        TLONG_sort_idx = np.expand_dims(TLONG_sort_idx, axis=extra_dims)\n",
    "    \n",
    "    ## now, do the actual sorting\n",
    "    data.values = sort(data, idx=TLONG_sort_idx)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36c31a-47d7-4d21-95b4-6cb7267aa1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(data, lat_range, lon_range=[0,360]):\n",
    "    \"\"\"select part of data in given longitude/latitude range\"\"\"\n",
    "\n",
    "    ## check if data is on the \"T\"-grid\n",
    "    on_Tgrid = \"TLONG\" in data.coords\n",
    "\n",
    "    ## handle trimming for T-grid\n",
    "    if on_Tgrid:\n",
    "\n",
    "        ## helper function to check if 'x' is in 'x_range'\n",
    "        isin_range = lambda x, x_range: (x_range[0] <= x) & (x <= x_range[1])\n",
    "        \n",
    "        ## get mask for data in given lon/lat range\n",
    "        in_lon_range = isin_range(data['TLONG'],lon_range)\n",
    "        in_lat_range = isin_range(data[\"TLAT\"], lat_range)\n",
    "        \n",
    "        in_lonlat_range = in_lon_range & in_lat_range\n",
    "\n",
    "        ## load to memory\n",
    "        in_lonlat_range.load()\n",
    "\n",
    "        ## Retain all points with at least one valid grid cell\n",
    "        x_idx = in_lonlat_range.any(\"nlat\")\n",
    "        y_idx = in_lonlat_range.any(\"nlon\")\n",
    "\n",
    "        ## select given points\n",
    "        return data.isel(nlon=x_idx, nlat=y_idx)\n",
    "\n",
    "    else:\n",
    "        return data.sel(lon=slice(*lon_range), lat=slice(*lat_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da576a3-aa2e-49cd-a188-592494fa0de5",
   "metadata": {},
   "source": [
    "# Load, trim, save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeed5d9d-72fc-4cc2-a946-e3d5fffd4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path to file server\n",
    "SERVER_FP = pathlib.Path(\"/vortexfs1/share\")\n",
    "\n",
    "## Specify folder location for saving trimmed data (\"./\" means current directory)\n",
    "save_fp = pathlib.Path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c4b441-a633-487c-b600-60e7522375f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "\n",
    "#this is janky because all these data products have different lat ranges; load one at a time and change trim\n",
    "\n",
    "## specify arguments common to all datasets\n",
    "kwargs = dict(varname=\"SST\", simulation_type=\"hist\", save_fp=save_fp, preprocess_func=preprocess)\n",
    "\n",
    "## Load the data\n",
    "lens_SH_S = load_ensemble(lat_range=[-90,-45], **kwargs)\n",
    "lens_SH_N = load_ensemble(lat_range=[-45,0], **kwargs)\n",
    "lens_NH_S = load_ensemble(lat_range=[0,45], **kwargs)\n",
    "lens_NH_N = load_ensemble(lat_range=[45,90], **kwargs)\n",
    "lens_EQ = load_ensemble(lat_range=[-30,30], **kwargs)\n",
    "lens_EQ2 = load_ensemble(lat_range=[-15,15], **kwargs)\n",
    "\n",
    "## Merge into single xr.Dataset\n",
    "lens = xr.merge(\n",
    "    [\n",
    "        lens_SH_S[\"SST\"].rename(\"SH_S\"),\n",
    "        lens_SH_N[\"SST\"].rename(\"SH_N\"),\n",
    "        lens_NH_S[\"SST\"].rename(\"NH_S\"),\n",
    "        lens_NH_N[\"SST\"].rename(\"NH_N\"),\n",
    "        lens_EQ[\"SST\"].rename(\"EQ\"),\n",
    "        lens_EQ2[\"SST\"].rename(\"EQ2\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "## load into memory\n",
    "lens.load();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b4a53a-4015-42cf-90ae-242ad8f468b3",
   "metadata": {},
   "source": [
    "# plot data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698bad8f-86d0-46a8-b4bb-d69ee07d00d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_setup_global(fig):\n",
    "    \"\"\"Plot Atlantic region\"\"\"\n",
    "\n",
    "    ## adjust figure size\n",
    "    fig.set_size_inches(5, 3)\n",
    "\n",
    "    ## specify map projection\n",
    "    proj = ccrs.PlateCarree(central_longitude=0)\n",
    "\n",
    "    ## get ax object\n",
    "    ax = plot_setup(\n",
    "        fig,\n",
    "        proj,\n",
    "        lon_range=[-180, 180],\n",
    "        lat_range=[-90, 90],\n",
    "        xticks=[-200, -100, 0, 100, 200],\n",
    "        yticks=[-90, -45, 0, 45, 90],\n",
    "    )\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2e1e52-52c7-4199-b5b3-235902342cfa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 3))\n",
    "\n",
    "## make background of trop. Pacific\n",
    "fig, ax = plot_setup_global(fig)\n",
    "\n",
    "## plot the data\n",
    "\n",
    "plot_data = ax.pcolormesh( \n",
    "    lens_SH_S.TLONG,\n",
    "    lens_SH_S.TLAT,\n",
    "    lens_SH_S.isel(member=0, time=0),\n",
    "    cmap=\"cmo.thermal\",\n",
    "    #vmax=30,\n",
    "    #vmin=15,\n",
    "    transform=ccrs.PlateCarree(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741e091f-d80c-44f4-8e67-42d1331dda8a",
   "metadata": {},
   "source": [
    "# timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e484d893-e960-4573-86d3-f6a00c21ad41",
   "metadata": {},
   "source": [
    "'''\n",
    " In LENS, you could isolate the effect of external forcing by averaging over all ensemble members.\n",
    " After ensemble-averaging, you could remove a linear trend (implicitly representing non-volcanic external\n",
    " forcing) and the seasonal cycle; hopefully, the resulting anomalies are dominated by the volcanic forcing. \n",
    " So the resulting anomalies would be your “residual” timeseries, called R(t) in the paper. \n",
    " Once you have this R(t), you could try to reproduce some of the plots in the \n",
    " paper (e.g., their Figs 3a and 3b).\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ece667-83e0-45a3-b3e8-45e3c16317e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_ensemble_trend(data, deg=4):\n",
    "    \"\"\"remove ensemble-mean polynomial trend of specified degree\"\"\"\n",
    "\n",
    "    ## compute ensemble mean\n",
    "    ensemble_mean = data.mean(\"member\")\n",
    "\n",
    "    ## subtrack from the data\n",
    "    return data - get_trend(ensemble_mean, deg=deg)\n",
    "\n",
    "def remove_ensemble_trend_by_season(data, deg=4):\n",
    "    \"\"\"remove ensemble trend for each calendar month separately \n",
    "    (so fit 12 separate trend curves)\"\"\"\n",
    "\n",
    "    ## helper function: detrends data with polynomial of spec'd degree\n",
    "    detrend_helper = lambda data : remove_ensemble_trend(data, deg=deg)\n",
    "\n",
    "    ## apply detrending to each month separately\n",
    "    return data.groupby(\"time.month\").map(detrend_helper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903bf2e0-7a9b-4195-a9a3-f8fb040908dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## which index to look at\n",
    "idx = \"SH_S\"\n",
    "\n",
    "#ensemble averaging\n",
    "ensemble_mean = lens.isel(member=slice(None,-2)).mean([\"member\"])\n",
    "\n",
    "#remove seasonal cycle\n",
    "data_deseason = ensemble_mean.groupby(\"time.month\") - ensemble_mean.groupby(\"time.month\").mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ab7ae0-1c93-458b-90d5-f17ef5196057",
   "metadata": {},
   "outputs": [],
   "source": [
    "## specify which region to plot\n",
    "region = \"NH_S\"\n",
    "\n",
    "## plot ensemble mean\n",
    "fig, ax = plt.subplots(figsize=(7, 3))\n",
    "\n",
    "ax.plot(\n",
    "    lens.time,\n",
    "    data_deseason[region],\n",
    "    c=\"r\",\n",
    "    lw=0.75,\n",
    "    zorder=2,\n",
    "    label=\"LENS Ensemble mean\",\n",
    ")\n",
    "\n",
    "\n",
    "## label\n",
    "ax.set_ylabel(r\"$^{\\circ}C$\")\n",
    "ax.legend()\n",
    "ax.set_title(region)\n",
    "ax.set_xlim([datetime.datetime(1920, 1, 1), datetime.datetime(2006, 12, 31)])\n",
    "\n",
    "## Add some markers\n",
    "ax.axvline(datetime.datetime(1982,1,1))\n",
    "ax.axvline(datetime.datetime(1963,1,1))\n",
    "ax.axvline(datetime.datetime(1991,1,1))\n",
    "ax.set_ylim([-.5,.5])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
