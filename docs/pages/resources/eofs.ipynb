{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de00f085-fe7a-4cc4-9dda-acf925750209",
   "metadata": {},
   "source": [
    "# Intro to EOFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc890d-7f8e-4d19-9141-75acf781cd70",
   "metadata": {},
   "source": [
    "## What are EOFs?\n",
    "\"Empirical Orthogonal Function\" (EOF) analysis is the name climate scientists use for \"principal component analysis\" (PCA). Intuitively, EOFs are the dominant spatial patterns in the data (we'll get to the precise mathematical definition of \"dominant pattern\" soon). They are \"empirical\" because they're estimated from data and \"orthogonal\" because each pattern identified in the analysis is required to be orthogonal to every other pattern (i.e., any two patterns have a spatial correlation of zero). Why use them? From a climate variability perspective, it can be helpful to think of the climate system having preferred \"modes\" of variability; in other words, preferred patterns of oscillation when perturbed{sup}`1`. Therefore a popular application of EOF analysis is to identify these \"modes\" of variability.\n",
    "\n",
    "\n",
    "{sup}`1`For what it's worth, I think about these modes like harmonics of a guitar string. When plucked, a guitar string prefers to oscillate at the fundamental frequency (1st harmonic), which has a wave length of the entire string. However, there are also \"overtones\", which correspond to oscillations at integer multiples of the fundamental frequency (e.g., 2nd harmonic has twice the frequency and half the wavelength of the 1st harmonic); see [the wikipedia page](https://en.wikipedia.org/wiki/Harmonic) for a visual representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdbf8cc-4eec-4334-a4ab-a04e6cb0dba3",
   "metadata": {},
   "source": [
    "### A visual demonstration\n",
    "Before explaining the math, let's look at a visual example. We'll start by generating some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81483437-3f84-47be-b8e4-b544c4c2ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## set plotting preferences and instantiate RNG\n",
    "sns.set(rc={\"axes.facecolor\": \"white\", \"axes.grid\": False})\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "## generate some data\n",
    "n = 100  # number of samples\n",
    "x = rng.normal(size=n)\n",
    "y = -0.4 * x + 0.25 * rng.normal(size=n)\n",
    "\n",
    "## plot the results\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## add data\n",
    "ax.scatter(x, y, s=10)\n",
    "\n",
    "## plot some vectors\n",
    "arrow_kwargs = dict(x=0, y=0, width=0.03, head_width=0.18)\n",
    "ax.arrow(dx=-0.6, dy=1, color=\"r\", **arrow_kwargs)\n",
    "ax.arrow(dx=0.5, dy=0.3, color=\"r\", **arrow_kwargs)\n",
    "ax.arrow(dx=-1, dy=0.4, color=\"k\", **arrow_kwargs)\n",
    "ax.arrow(dx=0.2, dy=0.5, color=\"k\", **arrow_kwargs)\n",
    "\n",
    "## plot axes\n",
    "ax.axvline(0, c=\"k\", ls=\"--\", lw=0.5)\n",
    "ax.axhline(0, c=\"k\", ls=\"--\", lw=0.5)\n",
    "\n",
    "## label plot\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xticks([-2, 0, 2])\n",
    "ax.set_yticks([-1, 0, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f356a0-40cf-4c5a-b3c9-8109708799c4",
   "metadata": {},
   "source": [
    "I've plotted two possible sets of orthogonal vectors, one in black and one in red. Which aligns \"better\" with the data? Intuitively, you might say the black set: the longer arrow seems to be aligned along the main axis of the data. This is exactly the goal of EOFs: to find the \"direction\" or \"pattern\" which \"aligns best\" with the data. In this 2-D case (we only have two coordinates) the pattern is a 2-D vector. If we had 3 coordinates, the vector would have 3-coordinates, and so on. If we have temperature data with coordiantes of $n_{lat}$ latitude points and $n_{lon}$ longitude points, the resulting vector will be $(n_{lon}\\times n_{lat})$ elements long; if we reshape this vector back into an array with dimensions of $(n_{lon}, n_{lat})$, then we get a spatial pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12119aa-7a07-4cc9-a6b3-e736a9be545f",
   "metadata": {},
   "source": [
    "## Math behind EOFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4364800a-dc08-4b9e-a539-b037e339fb05",
   "metadata": {},
   "source": [
    "Consider a set of $n$, $m$-dimensional vectors, concatenated into a data matrix $\\mathbf{X}$:\n",
    "\\begin{align}\n",
    "    \\mathbf{X} &= \\begin{bmatrix} \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_n \\end{bmatrix}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c76f62f-ebbc-46df-8cdc-4a36d497062e",
   "metadata": {},
   "source": [
    "We want to find the \"dominant\" $m$-dimensional pattern $\\mathbf{p}$. Mathematically, we define \"dominant pattern\" as the vector of length 1 whose projection onto the data yields the largest variance in time. In more detail:\n",
    "1. First, note that the \"projection\" of a data sample $\\mathbf{x}_i$ onto the pattern is the dot product $\\mathbf{p}\\cdot\\mathbf{x}_i$ (or equivalently, in matrix form, $\\mathbf{p}^T\\mathbf{x}_i$). Intuitively, the projection is the \"similarity\" between the pattern and the sample. A high value indicates a strong positive correlation, a low value indicates a strong negative correlation, and a value close to zero indicates a weak correlation. Note we can actually compute the pattern correlation explicitly as $\\text{corr}(\\mathbf{p},\\mathbf{x}_i) = \\frac{\\mathbf{p}\\cdot\\mathbf{x}_i}{\\sqrt{\\left(\\mathbf{p}\\cdot\\mathbf{p}\\right) \\left(\\mathbf{x_i}\\cdot\\mathbf{x}_i\\right)}}$.\n",
    "2. Then we can write the projection of each datapoint onto the pattern as: $\\mathbf{q}^T = \\mathbf{p}^T\\mathbf{X}$. Note that this projection vector, $\\mathbf{q}^T$, is a $1\\times n$ row vector, with a single entry for each of the $n$ samples (columns) in the dataset.\n",
    "3. The variance of the projected dataset is given by $\\text{var}(\\mathbf{q}) = \\frac{1}{n}\\mathbf{q}^T\\mathbf{q}=\\mathbf{p}^T\\mathbf{X}\\mathbf{X}^T\\mathbf{p}$. Note that the $m\\times m$ matrix $\\mathbf{C}_{xx}\\equiv \\frac{1}{n}\\mathbf{XX}^T$ is the covariance matrix of the data! Rewriting the variance in terms of this matrix, we have: $\\text{var}(\\mathbf{q})= \\mathbf{p}^T\\mathbf{C}_{xx}\\mathbf{p}$.\n",
    "4. Finally, define the leading EOF as the vector $\\hat{\\mathbf{p}}$ which maximizes the variance of $\\mathbf{q}$ subject to the constraint that $||\\hat{\\mathbf{p}}||=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be215b5-d395-4fba-b577-55c2c4ebd7c1",
   "metadata": {},
   "source": [
    "## Examples with real data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50199d-c70a-4f9f-8b91-b6d70dff4fc9",
   "metadata": {},
   "source": [
    "First, some functions to load, detrend, and plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58125b-9a1b-48af-b211-cd55b48df371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.ticker as mticker\n",
    "import cmocean\n",
    "\n",
    "\n",
    "def trim(data):\n",
    "    \"\"\"trim to lon/lat range\"\"\"\n",
    "\n",
    "    ## subset data in space (and downsample to 1-deg res.)\n",
    "    data_trimmed = data.sel(longitude=slice(280, 360, 4), latitude=slice(80, 20, 4))\n",
    "\n",
    "    ## get winter means\n",
    "    data_trimmed_winter = (\n",
    "        slp.resample(time=\"QS-DEC\").mean().isel(time=slice(None, None, 4))\n",
    "    )\n",
    "\n",
    "    return data_trimmed_winter\n",
    "\n",
    "\n",
    "def load_data(server_fp=None):\n",
    "    \"\"\"Load data from local directory if available; otherwise from server\"\"\"\n",
    "\n",
    "    ## filepath for saving data locally\n",
    "    save_fp = pathlib.Path(\"./slp_data.nc\")\n",
    "\n",
    "    ## check if data is saved locally\n",
    "    if save_fp.is_file():\n",
    "\n",
    "        data = xr.open_dataset(\"slp_data.nc\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        ## filepath of ERA5 data on server\n",
    "        fp_on_server = pathlib.Path(\n",
    "            \"cmip6/data/era5/reanalysis/single-levels/monthly-means/mean_sea_level_pressure\"\n",
    "        )\n",
    "\n",
    "        ## get full filepath\n",
    "        fp = pathlib.Path(server_fp) / fp_on_server\n",
    "\n",
    "        ## open data\n",
    "        data = xr.open_mfdataset(fp.glob(\"*.nc\"))\n",
    "\n",
    "        ## trim data\n",
    "        data = trim(data)\n",
    "\n",
    "        ## save to file\n",
    "        data.to_netcdf(save_fp)\n",
    "\n",
    "    ## convert data from Pa to hPa\n",
    "    hPa_per_Pa = 1 / 100\n",
    "    data_hPa = data * hPa_per_Pa\n",
    "\n",
    "    return data_hPa[\"msl\"]\n",
    "\n",
    "\n",
    "def get_trend_coefs(data, dim=\"time\", deg=1):\n",
    "    \"\"\"get coefficients for trend\"\"\"\n",
    "    return data.polyfit(dim=dim, deg=deg)[\"polyfit_coefficients\"]\n",
    "\n",
    "\n",
    "def get_trend(data, dim=\"time\", deg=1):\n",
    "    \"\"\"\n",
    "    Get trend for an xr.dataarray along specified dimension,\n",
    "    by fitting polynomial of degree 'deg'.\n",
    "    \"\"\"\n",
    "\n",
    "    ## Get coefficients for best fit\n",
    "    polyfit_coefs = get_trend_coefs(data=data, dim=dim, deg=deg)\n",
    "\n",
    "    ## Get best fit line (linear trend in this case)\n",
    "    trend = xr.polyval(data[dim], polyfit_coefs)\n",
    "\n",
    "    return trend\n",
    "\n",
    "\n",
    "def detrend(data, dim=\"time\", deg=1):\n",
    "    \"\"\"\n",
    "    Remove trend of degree 'deg' from data, along dimension 'dim'.\n",
    "    \"\"\"\n",
    "\n",
    "    return data - get_trend(data, dim=dim, deg=deg)\n",
    "\n",
    "\n",
    "def plot_setup(fig, projection, lon_range, lat_range, xticks=None, yticks=None):\n",
    "    \"\"\"Add a subplot to the figure with the given map projection\n",
    "    and lon/lat range. Returns an Axes object.\"\"\"\n",
    "\n",
    "    ## increase resolution for projection\n",
    "    ## (otherwise lines plotted on surface won't follow curved trajectories)\n",
    "    projection.threshold /= 1000\n",
    "\n",
    "    ## Create subplot with given projection\n",
    "    ax = fig.add_subplot(projection=projection)\n",
    "\n",
    "    ## Subset to given region\n",
    "    extent = [*lon_range, *lat_range]\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "\n",
    "    ## draw coastlines\n",
    "    ax.coastlines(linewidths=0.5)\n",
    "\n",
    "    ## add tick labels\n",
    "    if xticks is not None:\n",
    "\n",
    "        ## add lon/lat labels\n",
    "        gl = ax.gridlines(\n",
    "            draw_labels=True,\n",
    "            linestyle=\"-\",\n",
    "            alpha=0.1,\n",
    "            linewidth=0.5,\n",
    "            color=\"k\",\n",
    "            zorder=1.05,\n",
    "        )\n",
    "\n",
    "        ## specify which axes to label\n",
    "        gl.top_labels = False\n",
    "        gl.right_labels = False\n",
    "\n",
    "        ## specify ticks\n",
    "        gl.ylocator = mticker.FixedLocator(yticks)\n",
    "        gl.xlocator = mticker.FixedLocator(xticks)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_setup_atlantic(fig):\n",
    "    \"\"\"Plot Atlantic region\"\"\"\n",
    "\n",
    "    ## adjust figure size\n",
    "    fig.set_size_inches(5, 3)\n",
    "\n",
    "    ## specify map projection\n",
    "    proj = ccrs.Orthographic(central_longitude=-40, central_latitude=50)\n",
    "\n",
    "    ## get ax object\n",
    "    ax = plot_setup(\n",
    "        fig,\n",
    "        proj,\n",
    "        lon_range=[200, 360],\n",
    "        lat_range=[20, 80],\n",
    "        xticks=[150, -160, -110],\n",
    "        yticks=[-20, 0, 20],\n",
    "    )\n",
    "\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aa6df6-a9e7-4ec9-b030-6f640d49abe5",
   "metadata": {},
   "source": [
    "Now, load the data and compute anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0744e6d6-24f4-4f02-b9ad-67d36eb64a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load all data and detrend\n",
    "slp = load_data(server_fp=\"/Volumes\")\n",
    "slp_anom = detrend(slp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ffd82-241a-46cb-a454-4a1cf31ec3d8",
   "metadata": {},
   "source": [
    "### 2-D example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebd2946-3193-4d5e-8750-374ba837cc8b",
   "metadata": {},
   "source": [
    "Let's look at the SLP anomalies near Iceland and the Azores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44decb9c-2762-44c8-8ec4-184e5690736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xeofs as xe\n",
    "import pandas as pd\n",
    "\n",
    "## get SLP over iceland and azores\n",
    "slp_ice = slp_anom.interp(longitude=342.5, latitude=65)\n",
    "slp_azo = slp_anom.interp(longitude=332, latitude=38.5)\n",
    "\n",
    "## put iceland/azores data in single array\n",
    "slp_2d = xr.concat([slp_ice, slp_azo], dim=pd.Index([\"Iceland\", \"Azores\"], name=\"posn\"))\n",
    "\n",
    "## compute eofs\n",
    "model = xe.single.EOF(use_coslat=False)\n",
    "model.fit(slp_2d, dim=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6604825e-7a98-4d19-a33f-602739959bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot the results\n",
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## add data\n",
    "ax.scatter(slp_2d.sel(posn=\"Iceland\"), slp_2d.sel(posn=\"Azores\"), s=10)\n",
    "\n",
    "## plot EOFs\n",
    "arrow_kwargs = dict(x=0, y=0, width=0.15, head_width=0.9)\n",
    "\n",
    "# 1st EOF\n",
    "ax.arrow(\n",
    "    dx=10 * model.components().isel(mode=0, posn=0),\n",
    "    dy=10 * model.components().isel(mode=0, posn=1),\n",
    "    color=\"k\",\n",
    "    **arrow_kwargs\n",
    ")\n",
    "\n",
    "# 2nd EOF\n",
    "ax.arrow(\n",
    "    dx=5 * model.components().isel(mode=1, posn=0),\n",
    "    dy=5 * model.components().isel(mode=1, posn=1),\n",
    "    color=\"k\",\n",
    "    **arrow_kwargs\n",
    ")\n",
    "\n",
    "## plot axes\n",
    "ax.axvline(0, c=\"k\", ls=\"--\", lw=0.5)\n",
    "ax.axhline(0, c=\"k\", ls=\"--\", lw=0.5)\n",
    "\n",
    "## label plot\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.set_xlabel(\"Iceland SLP anomaly (hPa)\")\n",
    "ax.set_ylabel(\"Azores SLP anomaly (hPa)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b935641-7892-4960-90b3-649327ea3083",
   "metadata": {},
   "source": [
    "### Full spatial field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d0b306-9bdd-4734-b7f1-7a4471c36733",
   "metadata": {},
   "source": [
    "Compute EOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd59807d-9e6a-4402-b31b-2201412ad05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute eofs\n",
    "model = xe.single.EOF(use_coslat=True, n_modes=10)\n",
    "model.fit(slp_anom, dim=\"time\")\n",
    "\n",
    "## extract eofs and principal component timeseries ('pcs') for convenience\n",
    "eofs = model.components()\n",
    "pcs = model.scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3f787c-fc9b-48a4-9608-5af6a515bbeb",
   "metadata": {},
   "source": [
    "Plot spatial pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1bca45-a9b9-4055-a476-fb35d4d3f011",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "fig, ax = plot_setup_atlantic(fig)\n",
    "\n",
    "ax.pcolormesh(\n",
    "    eofs.longitude,\n",
    "    eofs.latitude,\n",
    "    eofs.isel(mode=0),\n",
    "    transform=ccrs.PlateCarree(),\n",
    "    cmap=\"cmo.balance\",\n",
    "    vmax=0.03,\n",
    "    vmin=-0.03,\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafa56cb-2478-48ef-b996-d0dd21b11c47",
   "metadata": {},
   "source": [
    "Plot timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e7dec-310b-4025-9ebf-a63225211c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## plot the data\n",
    "ax.plot(pcs.time.dt.year, pcs.isel(mode=0))\n",
    "\n",
    "## label\n",
    "ax.set_ylabel(\"Score (unitless)\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508a367a-fc79-4122-a2f5-60a87c25eb94",
   "metadata": {},
   "source": [
    "Explained variance ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06c5fd3-a22f-4d68-9014-7dea5a18f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "\n",
    "## plot the data\n",
    "ax.bar(model.explained_variance_ratio().mode, model.explained_variance_ratio())\n",
    "\n",
    "## label\n",
    "ax.set_ylabel(\"Fraction\")\n",
    "ax.set_xlabel(\"Mode #\")\n",
    "ax.set\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
